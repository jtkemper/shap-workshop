{
 "cells": [
  {
   "cell_type": "raw",
   "id": "5c1d09e7",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"01_shap_analysis\"\n",
    "author: \"JTK\"\n",
    "date: \"2025-12-03\"\n",
    "output:\n",
    "  pdf_document: default\n",
    "  html_document: default\n",
    "editor_options:\n",
    "  chunk_output_type: console\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34402f03",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# HOUSEKEEPING\n",
    "\n",
    "## Package Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269e541a",
   "metadata": {
    "name": "setup",
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "knitr::opts_chunk$set(echo = TRUE)\n",
    "\n",
    "library(tidyverse)\n",
    "library(lightgbm)\n",
    "library(here)\n",
    "library(caret)\n",
    "library(hydroGOF)\n",
    "library(fastshap)\n",
    "library(shapviz)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbce159",
   "metadata": {},
   "source": [
    "## Some global vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6440fbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Find your center\n",
    "### Tell R where to find us \n",
    "\n",
    "here::i_am(\"01_shap_analysis.Rmd\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b80ccde",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# DATA DOWNLOAD\n",
    "\n",
    "## Get model training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8d9c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# //////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "### Download provided .csv files that contain Phosphorus and Chloride obersvations\n",
    "### from 18 tributary gages of Lake Champlain, as well as many potential predictors\n",
    "### including dynamic discharge (daily discharge, antecedent weekly & montly discharge, etc.)\n",
    "### and several static parameters that could potentially drive water quality dynamics\n",
    "### See the Excel document within the input-data folder (/input-data/predictor_vars.xlsx)\n",
    "### for additional explanation of the predictor variables\n",
    "### Each file is 83 columns wide \n",
    "\n",
    "### Choose which constituent you are most interested in! Then make sure to edit\n",
    "### the relevant calls to make sure the code runs\n",
    "\n",
    "### The example below is for total phosphorus, which is the constituent \n",
    "### of primary concern within the Lake Champlain Basin \n",
    "### (as well as the broader New England, Mid-Atlantic, and Great Lakes region)\n",
    "### \n",
    "\n",
    "# //////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "### Read-in phosphorus & chloride data\n",
    "\n",
    "#### Phosphorus \n",
    "\n",
    "drivers <- read_csv(here(\"input\", \n",
    "                          \"final_dataframes_for_predictions\",\n",
    "                          \"tp_drivers.csv\")) \n",
    "\n",
    "#### Chloride \n",
    "\n",
    "# chlor_drivers <- read_csv(here(\"input\", \n",
    "#                             \"final_dataframes_for_predictions\",\n",
    "#                             \"tp_drivers.csv\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc94c2d",
   "metadata": {},
   "source": [
    "# DATA CLEANING & MANIPULATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d641a53",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# MODEL TRAINING\n",
    "\n",
    "## Feature Selection\n",
    "\n",
    "### Choose potential features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b9e13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# //////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "# There are many ways to perform feature selection, whether using random forest\n",
    "# and a selection \"philosophy\" (backwards or forwards selection) or using more\n",
    "# advanced techniques as evolutionary algorithms or Bayesian inference \n",
    "# However, that is not a focus of this workshop. Here, we will simply select variables\n",
    "# that our expertise suggests are likely to drive phosphorus dynamics \n",
    "\n",
    "# For this specific use case, this includes land cover, soils data, population data,\n",
    "# basin topography, various dynamic hydrology datasets, and season\n",
    "# We will then do a brief correlation analysis to remove high correlated variables\n",
    "\n",
    "# Feel free to pick your own variables from the list below based on your expertise\n",
    "# and curiousity\n",
    "# See Excel file in the github repo for variable explanations\n",
    "\n",
    "names(drivers) \n",
    "\n",
    "# Pick the features\n",
    "\n",
    "pred_feats <- c(\"water_year\",\n",
    "                   \"log_daily_q\",\n",
    "                   \"mean_prior_monthly_q\",\n",
    "                   \"delta_daily_q\",\n",
    "                   \"season\",\n",
    "                \"lulc_forest\",\n",
    "                \"lulc_developed\",\n",
    "                #\"lulc_agriculture\",\n",
    "                #\"lulc_water_wetland\",\n",
    "                \"all_100m_lulc_forest\",\n",
    "                \"all_100m_lulc_developed\",\n",
    "                #\"all_100m_lulc_agriculture\",\n",
    "                #\"all_100m_lulc_water_wetland\",\n",
    "                #\"drain_density_km_km2\",\n",
    "                \"tot_basin_slope\",\n",
    "                \"pct_ab_soils\",\n",
    "                #\"tot_p97\",\n",
    "                \"tot_ndams2010\",\n",
    "                \"tot_popdens10\"\n",
    "                #\"max_popdens10\",\n",
    "                #\"pct_drained_by_tile\"\n",
    "                       )\n",
    "\n",
    "### Now, check for correlations and remove correlated features\n",
    "#### Select just the features\n",
    "features_for_corr <- drivers %>% dplyr::select(any_of(pred_feats))\n",
    "\n",
    "#### Calculate correlations\n",
    "\n",
    "watershed_char_correlations <- cor(features_for_corr %>% \n",
    "                                     dplyr::select(!season),  \n",
    "                                   method = \"spearman\")\n",
    "\n",
    "#### Calculate statistical significance of correlations\n",
    "\n",
    "corr_p <- ggcorrplot::cor_pmat(watershed_char_correlations)\n",
    "\n",
    "coors_plot <- ggcorrplot::ggcorrplot(watershed_char_correlations, \n",
    "                                    p.mat = corr_p)\n",
    "coors_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7944d81e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Remove correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa291f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### Remove correlated features\n",
    "\n",
    "corrs_remove <- caret::findCorrelation(watershed_char_correlations, \n",
    "                                       cutoff = 0.7, \n",
    "                                       names = TRUE,\n",
    "                                       exact = TRUE,\n",
    "                                       )\n",
    "\n",
    "### And create the final predictor features dataframe\n",
    "\n",
    "drivers_selected <- drivers %>%\n",
    "  dplyr::select(tributary,\n",
    "                date,\n",
    "                log_conc,\n",
    "                any_of(pred_feats)) %>%\n",
    "  dplyr::select(!any_of(corrs_remove))\n",
    "\n",
    "\n",
    "\n",
    "# //////////////////////////////////////////////////////////////////////////////\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c005d5",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Split into training and testing datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fccc6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# //////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "# Here we split into training (and validation) datasets, and testing datasets\n",
    "# With time series data, it is important to retain the temporal structure\n",
    "# and so we divide datasets by year rather than a \"random\" split that is frequently\n",
    "# done in non-temporal data. This avoids skewing model performance towards \"good\"\n",
    "# which would occur if, for example, a model was predicting concentration at a timestep\n",
    "# for which it had observations on either side of that timestep\n",
    "# Of course, splits are ultimately dependant on your research questions. If you \n",
    "# want to use models to fill in gaps, for example, then a \"random\" split might \n",
    "# be the way to go. Here, are goals are interpretation, and so we do not \n",
    "# want to create a model that may \"learn\" temporal autocorrelation as the primary \n",
    "# driver of concentration rather than other underlying parameters and processes\n",
    "\n",
    "# //////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "### Split into training data\n",
    "\n",
    "train_data <- drivers_selected %>% filter(water_year < 2014)\n",
    "\n",
    "### And testing data\n",
    "### Note that we leave a buffer year to account for sample independence\n",
    "### and better ensure model generalizability \n",
    "\n",
    "test_data <- drivers_selected %>% filter(water_year > 2014)\n",
    "\n",
    "### Check how the splits shake out in terms of % of the entire sample\n",
    "\n",
    "### Test split % \n",
    "paste0(\"Train:\", round(nrow(train_data)/sum(nrow(train_data),nrow(test_data))*100, 0),\n",
    "                   \"% \",\n",
    "                   \"Test:\", round(nrow(test_data)/sum(nrow(train_data),nrow(test_data))*100, 0),\n",
    "                   \"%\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02668d97",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae47b1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "# First, set up the predictor dataframe as a matrix, which is required for the model\n",
    "# We also want to make sure to remove the the response variable (here, log_conc)\n",
    "# And a few other variables that are not predictors but we will later use to \n",
    "# to understand model outputs and assess model performance\n",
    "\n",
    "## Declare predictor variables\n",
    "\n",
    "preds <- data.matrix(train_data %>%\n",
    "                       dplyr::select(!c(log_conc,\n",
    "                                        date,\n",
    "                                        tributary)))\n",
    "\n",
    "## And the response variable\n",
    "\n",
    "response <- train_data$log_conc\n",
    "\n",
    "### Set up the environment - this is just preparing the dataset API to be used by lightgbm. \n",
    "### This is our training data\n",
    "\n",
    "train_lgbm <- lgb.Dataset(preds, label = response)\n",
    "\n",
    "\n",
    "### Declare the test data\n",
    "\n",
    "test_lgbm <- data.matrix(test_data %>%\n",
    "                       dplyr::select(!c(log_conc,\n",
    "                                        date,\n",
    "                                        tributary)))\n",
    "\n",
    "### Declare the hyperparameters\n",
    "### These are defaults but can be adjusted after the tuning process\n",
    "### which we won't explore here\n",
    "\n",
    "hyperparams <- list(objective = \"regression\",\n",
    "                    num_leaves = 31L,\n",
    "                    learning_rate = 0.1,\n",
    "                    min_data_in_leaf = 20L,\n",
    "                    num_threads = 10L)\n",
    "\n",
    "### Train the model\n",
    "                        \n",
    "set.seed(913)\n",
    "                        \n",
    "nutrient_model_lgbm <- lgb.train(hyperparams,\n",
    "                                 data = train_lgbm,\n",
    "                                 verbose = 1L,\n",
    "                                 nrounds = 500L)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4e3e5a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# MODEL TESTING\n",
    "\n",
    "## Make predictions on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41408585",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Predict with the model on test data\n",
    "\n",
    "nutrient_predicted <- predict(nutrient_model_lgbm, newdata = test_lgbm) %>%\n",
    "  as_tibble() %>% \n",
    "  rename(log_predicted_conc = 1)\n",
    "\n",
    "### Bind to observations\n",
    "predicted_observed <- bind_cols(nutrient_predicted,\n",
    "                                test_data %>%\n",
    "                                  rename(log_observed_conc = log_conc)) %>%\n",
    "  relocate(log_observed_conc, .after = log_predicted_conc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f1fad5",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Calculate errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e4e749",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "### Calculate errors\n",
    "\n",
    "predicted_observed <- predicted_observed %>%\n",
    "  mutate(predicted_conc = (10^log_predicted_conc)) %>%\n",
    "  mutate(observed_conc = 10^log_observed_conc) %>%\n",
    "  mutate(raw_err = predicted_conc - observed_conc) %>%\n",
    "  mutate(sqrerr = raw_err^2,\n",
    "         abs_error = abs(raw_err),\n",
    "         abs_pct_error = abs_error/predicted_conc\n",
    "                                 )\n",
    "\n",
    "\n",
    "### And some summary error metrics\n",
    "### Note that these are important to do by individual basin rather than \n",
    "### holistically due to the baseline comparisons in the metrics \n",
    "### (such as KGE)\n",
    "\n",
    "error_summary <- predicted_observed %>% \n",
    "  dplyr::group_by(tributary) %>%\n",
    "  summarise(mae = mean(abs_error),\n",
    "            rmse = sqrt(mean(sqrerr)),\n",
    "            nse = hydroGOF::NSE(predicted_conc, observed_conc),\n",
    "            kge = hydroGOF::KGE(predicted_conc, observed_conc),\n",
    "            pbias = hydroGOF::pbias(predicted_conc, observed_conc),\n",
    "            r2 = hydroGOF::R2(predicted_conc, observed_conc))\n",
    "\n",
    "### View median across all basins\n",
    "\n",
    "error_summary %>%\n",
    "  summarise(across(where(is.numeric), median))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cb2177",
   "metadata": {},
   "source": [
    "# FAST SHAP\n",
    "\n",
    "## Generate SHAP values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b16e241",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# //////////////////////////////////////////////////////////////////////////////\n",
    "# Here, we calculate SHAP values for predictions on the test dataset\n",
    "# As explained earlier, SHAP values are a game theory based approach that \n",
    "# provides insights into how exactly are being made. We can look at a variety\n",
    "# of different values and plots that quantify the contributions of features to \n",
    "# predictions, from the individual prediction scale to the full test dataset.\n",
    "# These can help reveal feature interactions and emphasize the phyiscal basis\n",
    "# of models. \n",
    "# //////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "\n",
    "### First create a wrapper function that enables prediction\n",
    "### on the test dataset\n",
    "\n",
    "p_func <- function(model, newdata){\n",
    "  \n",
    "  predict(model, as.matrix(newdata))\n",
    "  \n",
    "}\n",
    "\n",
    "### Then declare a baseline prediction to which SHAP values\n",
    "### are related.Traditionally, this is the the mean of the \n",
    "### target parameter in the training dataset\n",
    "### SHAP values reflect how much each feature changes the prediciton\n",
    "### relative to this baseline value \n",
    "\n",
    "baseline <- mean(train_data$log_conc)\n",
    "\n",
    "### Now, generate the shap values for the nutrient LightGBM model\n",
    "### This generates shap values for predictions on the test data\n",
    "\n",
    "ex_lgbm <- fastshap::explain(\n",
    "  nutrient_model_lgbm,\n",
    "  X = preds,\n",
    "  pred_wrapper = p_func,\n",
    "  newdata = test_lgbm,\n",
    "  adjust = TRUE,\n",
    "  exact = TRUE\n",
    "  \n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc07d305",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Make visualizaitons \n",
    "\n",
    "### Generate a shapviz visualization object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38710ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "shv <- shapviz::shapviz(nutrient_model_lgbm,\n",
    "                        #X = data.matrix(test_lgbm[60,]),\n",
    "                        #baseline = baseline,\n",
    "                        X_pred = test_lgbm\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e48d549",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Examine feature contributions to an individual prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298d2c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This is called a waterfall plot and helps to quantify how each feature\n",
    "# contributes to any given prediction and adjusts the prediction relative\n",
    "# to a baseline. Here we pick the fifth predictions, but you could do this \n",
    "# for any prediction of interest\n",
    "\n",
    "sv_waterfall(shv, row_id = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5859e9",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Visualize global values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe17140",
   "metadata": {},
   "outputs": [],
   "source": [
    "### This is what is known as a beeswarm plot and this provides a holistic \n",
    "### overview for how features contribute to predictions and how feature\n",
    "### contributions change as feature values change\n",
    "\n",
    "sv_importance(shv,\n",
    "              kind = \"beeswarm\",\n",
    "              viridis_args = list(),\n",
    "              show_numbers = TRUE ### Will show mean SHAP on grath\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da768b9a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Visualize mean shap, which is relatively analogous to traditional feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57923b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sv_importance(shv,\n",
    "              kind = \"bar\",\n",
    "              fill = \"tan4\",\n",
    "              show_numbers = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fd7cad",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### And now dependance plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112b05af",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\n",
    "### Dependance plots better show the relationship between individual feature \n",
    "### values and SHAP values. They can also help examine interactions between\n",
    "### features and how those influence SHAP values and, ultimately, model predictions\n",
    "\n",
    "sv_dependence(shv, \n",
    "              \"log_daily_q\",\n",
    "              color_var = \"lulc_forest\", # Help examine feature interaction \n",
    "              alpha = 0.5\n",
    "              #viridis_args = list()\n",
    "              )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee85cf8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "tags,name,-all",
   "main_language": "R",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
